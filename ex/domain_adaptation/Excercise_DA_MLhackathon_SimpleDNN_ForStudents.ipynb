{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d729525-b394-4876-95ae-f8744a0cc093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Domain Adaptation to train model-independent classifiers in High Energy Physics**\n",
    "\n",
    "In this notebook we want to build a simple DNN and train it using a typical HEP approach. Our dataset in this case will consist of only SM events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306bd8a5-356b-42ee-b8e5-9e6a65ba159b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Check the SM dataset\n",
    "\n",
    "This new dataset is very similar to the previous one, except that it is smaller and the BSM models have been dropped.\n",
    "\n",
    "Note that in this dataset all the events labelled as `isVBF` SM VBF events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07017091-bd4e-4e45-bd67-d25c73184360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617e5bc-906e-46fb-94c2-ef1d8d7dfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING COLAB, Uncomment and run the next lines and comment the next block; OTHERWISE leave all as is\n",
    "#%pip install pickle5\n",
    "#import pickle5 as pickle\n",
    "#!wget https://pandora.infn.it/public/123c9b/dl/dataset_SM.pkl\n",
    "#with open('dataset_SM.pkl', \"rb\") as fh:\n",
    "#  df = pickle.load(fh)\n",
    "# END COLAB BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa523b11-5657-4fca-99f3-2cbc19d37c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF NOT USING COLAB, please use the standard code below\n",
    "df = pd.read_pickle('https://pandora.infn.it/public/123c9b/dl/dataset_SM.pkl')\n",
    "# END NON COLAB BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086b6c9-868a-41f3-b465-1a35ff2abd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACK to standard flow\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cefb2-3e52-4686-96cd-1ea7e0b6bb66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How many events of each kind do we have?\n",
    "\n",
    "Let's know check how many events of each process we have in our dataset. In this case we opted for a balanced dataset composed by **VBF**, **ggH** and **BKG** processes in equal proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee14f2-2182-4eac-9b37-98a6c7ea17f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in df.columns[24:35]:\n",
    "    print (col, len(df[df[col]==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de953a2-333c-4832-abb4-9d7bec617ac0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define the a simple DNN\n",
    "\n",
    "We now want to build a simple DNN model. For the sake of clarity we maintain the same structure of the ADNN class, although it's much simpler now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e1568-6ab2-44f6-932c-22bfcef6abf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import optuna\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428079-de3f-41ff-b723-41c20cdb756b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork ( tf.Module ):\n",
    "    def __init__ (self, nEpochs, learning_rate, N_NODES, n_layers, n_features, n_outputsC=3):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer  = tf.optimizers.Adam (self.learning_rate)\n",
    "        self.nEpochs = nEpochs\n",
    "        self.N_NODES = N_NODES\n",
    "        self.n_layers = n_layers\n",
    "        self.n_features = n_features\n",
    "        self.n_outputsC = n_outputsC\n",
    "        self.weights = self.build (self.n_features, self.N_NODES)\n",
    "        \n",
    "                                \n",
    "    # Define the structure of the model\n",
    "    def build (self, n_input, N_NODES):\n",
    "\n",
    "        # Classifier model\n",
    "        self.model1 = Sequential()\n",
    "        self.model1.add(Dense (self.N_NODES, activation = 'relu', input_dim  = n_input))\n",
    "        for i in range(self.n_layers):\n",
    "            self.model1.add(Dense (self.N_NODES, activation = 'relu'))\n",
    "        self.model1.add(Dense (self.n_outputsC, activation = 'softmax',input_dim = self.N_NODES))      \n",
    "        \n",
    "        return self.model1.weights\n",
    "     \n",
    "    # Performs the epochs loop and the actual training.\n",
    "    # Monitors the training and validation loss functions, both for the classifier and the adversary.\n",
    "    # Returns the classifier categorical accuracy.\n",
    "    def fit (self, X, Y, X_val, Y_val, show_loss = False):\n",
    "        losses = []\n",
    "        losses_val = []\n",
    "\n",
    "        self.means = np.mean ( X, axis = 0)\n",
    "        self.sigmas = np.std ( X, axis = 0)\n",
    "\n",
    "        for iEpoch in tqdm.tqdm(range(self.nEpochs)):\n",
    "                l, l_val = self._train (X, Y, X_val, Y_val)\n",
    "                losses.append ( l )\n",
    "                losses_val.append ( l_val )\n",
    "\n",
    "        losses = np.array(losses)               \n",
    "        losses_val = np.array(losses_val)\n",
    "               \n",
    "        plt.plot (losses, color = \"c\", label='Training set')\n",
    "        plt.plot (losses_val, color ='tab:blue', label = \"Validation set\")\n",
    "        plt.xlabel (\"Epoch\"); plt.ylabel (\"Loss\")\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n",
    "        \n",
    "        ca = tf.keras.metrics.CategoricalAccuracy()\n",
    "        ca.update_state(Y, self.predict_proba(X))\n",
    "        \n",
    "        return ca.result().numpy()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _train (self, X, Y, X_val, Y_val):\n",
    "        Y_true = tf.cast (Y, tf.float32)\n",
    "        Y_true_val = tf.cast (Y_val, tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as gt:\n",
    "            #gt.watch ( self.weightsC )\n",
    "            Y_hat = self.predict_proba (X)\n",
    "            Y_hat_val = self.predict_proba (X_val)\n",
    "            \n",
    "            ## Training set\n",
    "            # Use the categorical cross-entropy as loss function for the classifier\n",
    "            cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "            loss = tf.reduce_mean ( cce( Y_true, Y_hat ) )\n",
    "            \n",
    "            ## Validation set\n",
    "            cce_val = tf.keras.losses.CategoricalCrossentropy()\n",
    "            loss_val = tf.reduce_mean (cce_val( Y_true_val, Y_hat_val ) )\n",
    "            \n",
    "            # Compute the gradient of the overall loss with respect to the classifier weights\n",
    "            gradients = gt.gradient ( loss, self.weights )\n",
    "\n",
    "        # Apply the gradients\n",
    "        self.optimizer.apply_gradients ( zip(gradients, self.weights) )\n",
    "        \n",
    "        return loss, loss_val\n",
    "\n",
    "    \n",
    "    # Applies a pre-processing to the input features and returns the classifier representation.\n",
    "    @tf.function\n",
    "    def predict_proba (self, X):\n",
    "        ppX = (X - self.means)/self.sigmas\n",
    "        return  tf.clip_by_value ( self.model1 (ppX) , 1e-7, 1. - 1e-7 )\n",
    "    \n",
    "    def save_weights(self, model_name):\n",
    "        self.model1.save_weights(model_name+'_weights_1')\n",
    "    \n",
    "    def load_weights(self, model_name):\n",
    "        self.model1.load_weights(model_name+'_weights_1')\n",
    "        \n",
    "    def save_model(self, model_name):\n",
    "        self.model1.save(\"saved_models/\"+model_name+\"_1\")\n",
    "\n",
    "    def reset_optimizers(self):\n",
    "        self.optimizer  = tf.optimizers.Adam (self.learning_rate)\n",
    "        \n",
    "    def set_epochs(self, epochs):\n",
    "        self.nEpochs = epochs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefa974-ff1b-4b7f-9cec-fab1ad00e341",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training of the simple DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1543a3-3862-4f5b-b72f-dfe4bb6091b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract the input features and labels from the SM dataset and split in training (80%) and validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906fb5f-22cd-49b3-a204-03b09e241786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "\n",
    "features = df.columns[:24]\n",
    "\n",
    "NDIM = len(features)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "# Perform the splitting and define training and validation datasets\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_val = df[~msk]\n",
    "\n",
    "X = df_train.values[:,0:NDIM]\n",
    "Y = df_train.values[:,NDIM:NDIM+3] # isVBF, isGGH, isBKG\n",
    "\n",
    "X_val = df_val.values[:,0:NDIM]\n",
    "Y_val = df_val.values[:,NDIM:NDIM+3] # isVBF, isGGH, isBKG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cca561-115d-475d-97a8-c8c00a48adcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the simple DNN model and define the needed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842102f-5a82-4598-a1d9-15d43e3106a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dnn = SimpleNeuralNetwork(500, learning_rate=0.0001, N_NODES=50, n_layers=8, n_features=X.shape[1])\n",
    "\n",
    "# Save initial set of weights (before training) to re-initialize the ADNN in later steps.\n",
    "# Useful if we want to restart always from the same starting point during the optimization studies.\n",
    "dnn.save_weights(\"my_simpleDNN_model_init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c16e3-5f49-4a22-b8b2-afb5163e3df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af243aa-5a2b-4766-a4b2-1a90cf97e0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = dnn.fit (X.astype(np.float32), Y.astype(np.float32), X_val.astype(np.float32), Y_val.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b51da-1c0e-4e97-bfcd-00275d78ad10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# How do we quantify the simple DNN performance\n",
    "\n",
    "To compare with the ADNN approach, we want to know the accuracy and the performance of the simple DNN on the models belonging to the target domain, i.e. BSM models.\n",
    "\n",
    "To achieve this, let's resume the ADNN dataset and use it to evaluate the simple DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91771967-c223-4c92-b798-f5813831600d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot the probability distributions for the three output nodes of C\n",
    "\n",
    "**YOUR TURN: as for the ADNN, redo the same plots also for the simple DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6751c8-6356-449e-ab2c-a1578a06471b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948701c-79f6-4336-9129-7999b5dc4f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot the probability distributions for labelled events\n",
    "\n",
    "**YOUR TURN: as for the ADNN, redo the same plots also for the simple DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548cf63-6098-4d0c-b36e-7d992cfa2768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351e2da-4060-45e3-926a-699bcc328954",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's calculate the accuracy\n",
    "\n",
    "**YOUR TURN: as for the ADNN, check the categorical accuracy of the simple DNN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008af26-6dde-40b0-b8f6-fd5127d905ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda42ea-9945-4292-ba2f-64be8351f6fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot the probability distributions of different VBF signal models\n",
    "\n",
    "Note that to do this we reload the original dataset that contained also the BSM events, and we evaluate the simple DNN on this dataset.\n",
    "\n",
    "**YOUR TURN: after reloading the model used for the ADNN, plot the VBF output distribution for all the BSM models. What do you expect in this case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c96121-6e30-417d-a8ba-07636d6ad1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IF NOT USING COLAB, USE LINES BELOW; OTHERWISE COMMENT THEM AND UNCOMMENT NEXT BLOCK\n",
    "df_bsm = pd.read_pickle('https://pandora.infn.it/public/488317/dl/dataset_DA.pkl')\n",
    "\n",
    "#BLOCK FOR COLAB\n",
    "#!wget https://pandora.infn.it/public/488317/dl/dataset_DA.pkl\n",
    "#with open('dataset_DA.pkl', \"rb\") as fh2:\n",
    "#  df_bsm = pickle.load(fh2)\n",
    "#END NON COLAB BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd6706-6035-4109-ac15-395a0e1f8561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0753820-4fec-41c2-8321-124298225b25",
   "metadata": {},
   "source": [
    "Wow, that's really bad..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc74f1-3f98-4e95-9974-5231a96345aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summary of the simple DNN performance\n",
    "\n",
    "Let's finish this part by computing the DNN accuracy on BSM models, as well ad the metrics used to evaluate the level of agreement between SM and BSM distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed53cf-6788-4215-98f0-5f56a48e6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "#from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize = 16)\n",
    "    plt.yticks(tick_marks, classes, fontsize = 16)\n",
    "\n",
    "    thresh = cm.max() / 1.2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",fontsize=10)\n",
    "\n",
    "    plt.xlabel(\"Predicted label\", fontsize=16)\n",
    "    plt.ylabel(\"True label\", fontsize=16)\n",
    "\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4d480-a654-4b65-9809-9f91a0e4544f",
   "metadata": {},
   "source": [
    "**YOUR TURN: use the `summary()` function defined in the other notebook and adapt it to produce the same outputs for the simple DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e487fcc-aa6e-4072-9806-5a0a21168277",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
