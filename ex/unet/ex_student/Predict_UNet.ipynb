{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae1d48d-c04a-473c-8859-9a1da5a1315e",
   "metadata": {},
   "source": [
    "# Prediction computation and evaluation\n",
    "\n",
    "Once the neural network has been trained, one can use the fitted weights to infer a prediction of an image that the model has never seen. In this stage we can test the predictive power of the network by comparing the inferred results with previously labelled data. In this notebook we will load define a variable containing the model where we can load the trained weight and subsequently use our test data to evaluate our model, once this is done we store the results in an array so we can plot which image has the best prediction and compare it with the worst prediction. To define which is the most/least accurate prediction we'll have to use some metrics, here we'll use the dice metric. Furthermore, before doing any inference we will check how the training went so that we may have an idea on the final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b597abec-4745-48d9-bc35-2ba7ab99ec74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Data_Generator.DataGenerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9019/1290382241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TF_CPP_MIN_LOG_LEVEL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mData_Generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataGenerator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoss_Metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLossMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNet_Arch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNET_architecture\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Data_Generator.DataGenerator'"
     ]
    }
   ],
   "source": [
    "# import the modules and clear any previous keras session to avoid possible errors\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from glob import glob\n",
    "import tensorflow.keras.backend as K\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import Data_Generator.DataGenerator as DG\n",
    "import Loss_Metrics.LossMetrics \n",
    "import UNet_Arch.UNET_architecture as unet\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7c4f7-4eb9-4966-bb93-de8a2923cdbd",
   "metadata": {},
   "source": [
    "## Evaluate the training\n",
    "\n",
    "To do this we'll read the history.json file and extract the information to be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec1e27-b271-4553-9758-7d5be3b027cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = json.load(open(\"history.json\", 'r'))\n",
    "history_dict = json.loads(r\"{}\".format(history_dict).replace(\"\\\"\",\"\").replace(\"'\",\"\\\"\"))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,9))\n",
    "epochs = range(len(history_dict[\"loss\"]))\n",
    "ax[0].plot(epochs, history_dict[\"loss\"], label=\"loss\")\n",
    "ax[0].plot(epochs, history_dict[\"val_loss\"], label=\"val_loss\")\n",
    "ax[1].plot(epochs, history_dict[\"DSC\"], label=\"DSC\")\n",
    "ax[1].plot(epochs, history_dict[\"val_DSC\"], label=\"val_DSC\")\n",
    "\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[1].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "ax[1].set_ylabel(\"DSC\")\n",
    "\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b1498-5adb-46dc-9f06-a4b85853c7cd",
   "metadata": {},
   "source": [
    "At this point you may notice two things, that the training has gone in overfitting if validation and training metrics separate at the last epoch or that it has not gone into overfitting if the metrics follow the same trend. If the validation metric has a reasonable value, one can go forward and try to infer results on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dda8c4-27e8-4eaa-93d5-11ca30486a47",
   "metadata": {},
   "source": [
    "## Import the test data\n",
    "\n",
    "First of all we'll import the data against which we'll evaluate the network inference. \n",
    "\n",
    "NB: the size of the image should be defined correctly and also the indexes of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf59fd8-d191-4b90-9067-40dc89248809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 128\n",
    "\n",
    "path='./Shezen_{:d}{:d}/'.format(dim,dim) # input images path\n",
    "\n",
    "data_path=os.path.join(path,\"*\")#'../../shared/postuma/Shezen_{:d}{:d}/*'.format(dim,dim)\n",
    "data_list = [os.path.basename(f) for f in glob(data_path)] \n",
    "\n",
    "test_list = data_list[590:]\n",
    "\n",
    "print(test_list)\n",
    "\n",
    "model = unet.U_net((dim,dim,1))\n",
    "# NB: load your weights file\n",
    "model.load_weights(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9f2c0-0f40-4dd8-aba9-56d7a1307ab1",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Here we'll perform the prediction and store the results.\n",
    "\n",
    "NB: the loaded numpy array has the following dimension (x,y,c), where x and y are the size of the image while c the number of channels. The keras ```predict``` method reads multiple images at once, in this example we'll iterate through the images and infer the results one image at a time. So in this case we should pass to ```predict```and image with shape (1,x,y,c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187bdc9-36d4-457f-b54c-87faebcb998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = []\n",
    "bdsc = []\n",
    "pred = []\n",
    "\n",
    "for patient in test_list:\n",
    "    print(patient)\n",
    "    ''' Read the test data, pre-process them in order to be used by the U_Net\n",
    "    and make the prediction.\n",
    "    '''\n",
    "\n",
    "    prediction = model.predict(img)\n",
    "    prediction = np.round(prediction)\n",
    "    print(prediction.shape)\n",
    "    pred.append(prediction)\n",
    "    dice.append(Loss_Metrics.LossMetrics.DSC(label.astype('float32'), prediction.astype('float32')))\n",
    "    bdsc.append(Loss_Metrics.LossMetrics.border_dice(label[0,:,:,0],prediction[0,:,:,0]))\n",
    "    \n",
    "\n",
    "avg_dice = np.mean(dice)\n",
    "std_dice = np.std(dice)\n",
    "avg_sdsc = np.mean(bdsc)\n",
    "std_sdsc = np.std(bdsc)\n",
    "\n",
    "print('***************************************+')\n",
    "print('Mean DSC', avg_dice, '+-', std_dice)\n",
    "print('Mean BDSC', avg_sdsc, '+-', std_sdsc)\n",
    "print('***************************************+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b5bf9-4d03-484c-bc16-ec122d10598c",
   "metadata": {},
   "source": [
    "Now we'll plot the best and worst case for different metrics (i.e. dice and border dice). To do this we define two function which will find the index of best and worst case and a third function which plots the predicted and real mask next to each other to apreciate the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d450d-4696-4bdf-aaf4-f774367bab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minvalue(inputlist):\n",
    " \n",
    "    #get the minimum value in the list\n",
    "    min_value = min(inputlist)\n",
    " \n",
    "    #return the index of minimum value \n",
    "    min_index=inputlist.index(min_value)\n",
    "    return min_index\n",
    "\n",
    "def get_maxvalue(inputlist):\n",
    " \n",
    "    #get the minimum value in the list\n",
    "    min_value = max(inputlist)\n",
    " \n",
    "    #return the index of minimum value \n",
    "    min_index=inputlist.index(min_value)\n",
    "    return min_index\n",
    "\n",
    "def plot_compare(index, metric=\"\"):\n",
    "    fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "    \n",
    "    fig.suptitle(metric)\n",
    "\n",
    "    data = np.load(predict_path + test_list[index])\n",
    "    mask_true = np.ma.masked_where(data[...,1]<1,data[...,1])\n",
    "    mask_pred =  np.ma.masked_where(pred[index][0,...,0]<1,pred[index][0,...,0])\n",
    "\n",
    "    ax[0].imshow(data[...,0],cmap=\"bone\")\n",
    "    ax[0].imshow(mask_true,cmap=\"Reds\", vmin=0,vmax=1)\n",
    "    ax[1].imshow(data[...,0],cmap=\"bone\")\n",
    "    ax[1].imshow(mask_pred,cmap=\"Reds\", vmin=0,vmax=1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d176f08-e685-46d6-9acf-4e5722a8ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dsc = get_minvalue(dice)\n",
    "plot_compare(min_dsc, metric=\"dice = {}\".format(dice[min_dsc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21a99f-3ea4-4e4c-a4cd-b72ca15fd5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dsc = get_maxvalue(dice)\n",
    "plot_compare(max_dsc, metric=\"{}\".format(dice[max_dsc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339e80a-e87b-48d9-bc73-8fc83ab8f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bdsc = get_minvalue(bdsc)\n",
    "plot_compare(min_bdsc, metric=\"border dice = {}\".format(bdsc[min_bdsc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6a322-ed06-441f-86d6-c1b157631450",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bdsc = get_maxvalue(bdsc)\n",
    "plot_compare(max_bdsc, metric=\"border dice = {}\".format(bdsc[max_bdsc]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
